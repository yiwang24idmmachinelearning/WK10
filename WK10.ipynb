{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10\n",
    "\n",
    "ML review, Ingredients for ML models, Intro to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "We've seen a couple of different methods of doing classic machine learning for classification, regression and clustering tasks.\n",
    "\n",
    "Despite operating on different types of data, using different algorithms and serving different purposes, all of the methods we've seen follow a common set of operations, or, pipeline:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/ML_00.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data**: we start with a collection of files or numbers that we process, analyze, visualize and study to understand their content and relationships. We then split this data into $2$ separate datasets, one for training our algorithm and another to test how it performs omn data it hasn't seen.\n",
    "\n",
    "- **Algorithm**: these are the mathematical operations that get performed on the data to extract patterns and relationships between our data points. The algorithm chosen depends on the type of task we are trying to accomplish.\n",
    "\n",
    "- **Cost Function**: in order for the algorithm to learn anything we have to guide it by telling it how close it gets to correct answers. This is particularly important for supervised learning, where the algorithm gets the data to operate on and the correct answer for the task. Some algorithms have built-in cost functions, others take a cost function as a parameters, but either way, this is the function that the algorithm uses to adjust its parameters.\n",
    "\n",
    "- **Evaluation Function**: once our algorithm builds a model from the training data, the evaluation function is what we use to measure how well the model performs on the test dataset. The evaluation function can be the same as the cost function, but is usually a little bit more legible. Where the cost function can be a complex mix of formulas, each meant to guide the algorithm navigate tradeoffs when picking parameters, the evaluation function is meant to validate whether our choice of algorithm, cost function and data is sufficient for our overall goals.\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/ML_01.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of each of these components:\n",
    "- **Data Processing**:\n",
    "  - [Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "  - [Scaling](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "  - [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "- **Cost Function**:\n",
    "  - [Distance](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html)\n",
    "  - [MSE](https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
    "  - [Class Likelihood](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.class_likelihood_ratios.html)\n",
    "\n",
    "- **Algorithm**:\n",
    "  - [Linear Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "  - [Random Forest](https://scikit-learn.org/stable/modules/ensemble.html#random-forests-and-other-randomized-tree-ensembles)\n",
    "  - [SVMs](https://scikit-learn.org/stable/modules/svm.html)\n",
    "  - [Clustering](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "- **Evaluation Function**:\n",
    "  - [Accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "  - [Precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "  - [Other metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/ML_02.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "**_Evaluation Metrics_** are calculations that we perform on the predictions from our training and test data. They are related to the Cost/Distance calculations that happen during the training process, but they are supposed to be a little more expressive, concrete, practical and more representative of the actual application of our model. Some evaluation metrics can be more qualitative than quantitative.\n",
    "\n",
    "We use these evaluation metrics to measure how well our model is performing and guide us on how to improve it.\n",
    "\n",
    "Our models will usually have just one cost function that is minimized during training, using the data available to decrease the distances between its predictions and the actual desired outputs. On the other hand, we can have multiple evaluation metrics for any given model, and unlike our cost function, the result of the evaluation functions don't get automatically fed back into the model to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The **_Confusion Matrix_** graph above is an example of an evaluation metric, one that is specifically used for measuring how well a classification model performs.\n",
    "\n",
    "It's a very elegant way of displaying which classes our model can classify, which classes it confuses with other classes, and if there are any classes that it prefers to predict. There's actually a lot of information in that graph, that can tell us different things about our model and its potential shortcomings.\n",
    "\n",
    "Let's look at a simpler **_Confusion Matrix_** and break down all the information that it displays:\n",
    "\n",
    "<img src=\"./imgs/confusion2.jpg\" height=\"350px\" />\n",
    "\n",
    "The above matrix for a fictitious model shows that we are interested in classifying things into two categories, with labels $0$ and $1$. Our data set contains $7$ instances of the $0$ class (sum of first row), and $10$ instances of class $1$ (sum of second row).\n",
    "\n",
    "Our model correctly predicted $5$ of the $7$ instances of class $0$; while correctly predicting $7$ of the $10$ instances of class $1$. These are called the `True` classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "\n",
    "We can use these numbers to compute the **_Accuracy_** of our model, or, how often it gets the correct answer:\n",
    "\n",
    "$\\displaystyle Accuracy = \\frac{T_0 + T_1}{Total\\ Instances}$\n",
    "\n",
    "And for our example model:\n",
    "\n",
    "$\\displaystyle Accuracy = \\frac{5 + 7}{5+2+3+7} = \\frac{12}{17} \\approx 0.7059 = 70.6\\%$\n",
    "\n",
    "Besides the **_Accuracy_** of our model, we can also compute **_Precision_** and **_Recall_** characteristics for each of the classes in our model.\n",
    "\n",
    "#### Precision\n",
    "\n",
    "**_Precision_** is the proportion of our predictions that were actually correct for any given class. This number will be low if our model is biased to predict any of the classes.\n",
    "\n",
    "$\\displaystyle Precision_c = \\frac{T_c}{T_c + F_c}$\n",
    "\n",
    "$\\displaystyle Precision_0 = \\frac{5}{5 + 3} = 0.625 = 62.5\\%$ $\\hspace{5em}$\n",
    "$\\displaystyle Precision_1 = \\frac{7}{7 + 2} \\approx 0.7778 = 77.78\\%$\n",
    "\n",
    "#### Recall \n",
    "\n",
    "**_Recall_** is the proportion of the true data labels that were predicted correctly for each class. This number will be low if our model can't really recognize one of the classes.\n",
    "\n",
    "$\\displaystyle Recall_c = \\frac{T_c}{Total_c}$\n",
    "\n",
    "$\\displaystyle Recall_0 = \\frac{5}{5 + 2} \\approx 0.7143 = 71.43\\%$ $\\hspace{5em}$\n",
    "$\\displaystyle Recall_1 = \\frac{7}{7 + 3} = 0.7 = 70\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Classes\n",
    "\n",
    "The exact same calculations can be done on any confusion matrix, independent of the number of classes in our model.\n",
    "\n",
    "Consider the following matrix for a classification model with $3$ classes:\n",
    "\n",
    "<img src=\"./imgs/confusion3.jpg\" height=\"350px\" />\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "This is still $\\frac{correct}{total}$. So, for our model above:\n",
    "\n",
    "$\\displaystyle Accuracy = \\frac{5 + 7 + 6}{5+2+1+2+7+2+3+0+6} = \\frac{18}{28} \\approx 0.6428 = 64.3\\%$\n",
    "\n",
    "#### Precision\n",
    "\n",
    "The proportion of our predictions that were actually correct for any given class.\n",
    "\n",
    "$\\displaystyle Precision_0 = \\frac{5}{5 + 2 + 3} = 0.5 = 50\\%$ $\\hspace{5em}$\n",
    "$\\displaystyle Precision_1 = \\frac{7}{7 + 2 + 0} \\approx 0.7778 = 77.78\\%$\n",
    "\n",
    "$\\displaystyle Precision_2 = \\frac{6}{6 + 2 + 1} \\approx 0.6667 = 66.67\\%$\n",
    "\n",
    "#### Recall\n",
    "\n",
    "The proportion of the true data labels that were predicted correctly for each class.\n",
    "\n",
    "$\\displaystyle Recall_0 = \\frac{5}{5 + 2 + 1} = 0.625 = 62.5\\%$ $\\hspace{5em}$\n",
    "$\\displaystyle Recall_1 = \\frac{7}{7 + 2 + 2} \\approx 0.6364 = 63.64\\%$\n",
    "\n",
    "$\\displaystyle Recall_2 = \\frac{6}{6 + 0 + 3} \\approx 0.6667 = 66.67\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations\n",
    "\n",
    "Ideally we would have $100\\%$ for all of these metrics, but that's rarely the case.\n",
    "\n",
    "They become useful when we have to consider tradeoffs between having models that are too sensitive and eager to predict certain classes, which might lead to low **_precision_**, versus models that don't predict certain classes ever, which would lead to low **_recall_**.\n",
    "\n",
    "For example, if we are working on a face recognition system that will be used to unlock people's devices, we can probably live with lower **_recall_**, as long as our **_precision_** is high. This would mean that the system wouldn't always recognize certain faces easily, making it hard for some people to unlock their devices, but, on the other hand, the high **_precision_** would mean that it would be rare for the system to unlock the device for the wrong face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "We've seen this a few times now, and it was usually in the form of a function called `fit()`.\n",
    "\n",
    "Training, or fitting, is the process by which the algorithm combines our data and our cost function to produce a model.\n",
    "\n",
    "The process can be summarized like this:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/training_00.jpg\" width=800px />\n",
    "\n",
    "Every model prediction is compared to the correct output value available in the training data, and the difference between prediction and true value is used to adjust the model's mathematical parameters.\n",
    "\n",
    "If we were to visualize this process over time for a linear regression model and a clustering model, we might see something like this, where the model's performance improves as it uses more data to adjust its parameters:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/training_01.jpg\" width=800px />\n",
    "<br>\n",
    "<img src=\"./imgs/training_02.jpg\" width=800px />\n",
    "\n",
    "\n",
    "Some of the models we've seen so far have \"_closed-form_\" solutions. This means that they're able to look at all of the training data at the same time and, using basic algebraic operations $(+$, $-$, $\\times$, $\\div)$ and matrix algebra, come up with optimal parameters almost instantaneously. This is the case for `PCA` and `Linear Regression` models.\n",
    "\n",
    "This is both a strength and a weakness of these types of models. They train really fast and work really well, as long as we're working with not-so-large datasets for specific tasks. As the variation in our data becomes too large, and the relations we are trying to model grow in complexity, these models start to fall short in quality, or become impractical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "This has been the most popular method for creating models that can handle very diverse types of data, while not requiring a lot of customization and manual parameter-tuning.\n",
    "\n",
    "The process for training a neural network is exactly the same:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_00.jpg\" width=800px />\n",
    "\n",
    "Except, instead of using very specific matrix operations and algebraic expressions $(+$, $-$, $\\times$, $\\div)$, all of the calculations (processing, predictions, etc) are done using A LOT of very generic, very simple, computational elements called \"_neurons_\" or \"_perceptrons_\":\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_01.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "These are supposed to mimic how actual brain neurons work: they fire and propagate signals depending on the combination and strength of signals present at their inputs.\n",
    "\n",
    "The operation of a single neuron is quite simple:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_02.jpg\" width=800px />\n",
    "\n",
    "They first perform a weighted sum of their input signals:\n",
    "\n",
    "$\\displaystyle Z = w_A \\cdot A + w_B \\cdot B + w_C \\cdot C + ... $\n",
    "\n",
    "and then, an _activation function_ determines if and how the neuron fires, based on this weighted sum and a threshold value.\n",
    "\n",
    "This looks a lot like the function that we try to optimize during linear regression:\n",
    "\n",
    "$\\displaystyle Y = \\beta_0 \\cdot x_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + ... $\n",
    "\n",
    "The difference is that before we had $N$ parameters for $N$ features, and with a neural network we'll have around $N$ parameters per node and about $N$ nodes (one for each feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks\n",
    "\n",
    "Despite this increase in the number of parameters, neural networks are beneficial because they are modular, and relatively simple to extend when needed.\n",
    "\n",
    "Where previously we needed to know very specific strategies for dealing with different types of problems (adding non-linear functions to linear regression, or dimensionality reduction with PCA, or normalization), with neural networks, we can just add more layers and more nodes:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_03.jpg\" width=800px />\n",
    "\n",
    "Another benefit of neural networks is that the same architecture can be used for different tasks by just changing the activation function of the nodes in the last layers.\n",
    "\n",
    "For regression tasks we remove any activation function and just use the node's weighted sum. For classification tasks, we use something called a `softmax()` function that turns the weighted sum into a likelihood value:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/softmax_00.jpg\" width=800px />\n",
    "\n",
    "Other tasks, like image detection, segmentation, generation or encoding, just use a combination of these two types of output nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Networks\n",
    "\n",
    "Training neural networks is a process similar to the `fit()` step we've seen in other models:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_00.jpg\" width=800px />\n",
    "\n",
    "But, because they tend to have A LOT more parameters, training takes A LOT more data and A LOT more time.\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_04.jpg\" width=800px />\n",
    "\n",
    "The cost function is still responsible for starting the process of adjusting the parameters in a neural network, using a method called _Backpropagation_.\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_05.jpg\" width=800px />\n",
    "\n",
    "For every record (or batch of records) in the training dataset, the cost function informs each node how their parameters could be changed in order to decrease the error on the output:\n",
    "\n",
    "<br>\n",
    "<img src=\"./imgs/nn_06.jpg\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bit More Detail\n",
    "\n",
    "<img src=\"./imgs/slides_00.jpg\" width=800px />\n",
    "\n",
    "<a href=\"https://docs.google.com/presentation/d/1ppf-nxKS9QKvuNrx37SVkiAs4nk8qJv9JZz7WhhwjFo/\">SLIDES</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "We'll be using the [PyTorch](https://pytorch.org/) library for working with Neural Networks.\n",
    "\n",
    "Before we start building, training, tuning models, we have to learn a little bit about [Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)!\n",
    "\n",
    "Tensor is a fancy word for multi-dimensional list. They are very much like lists, where they keep a sequence of number values, or a sequence of other tensors. They are a little bit more picky than lists because they require all members to be of the same _type_ (all integers, or all floats, etc), and they don't like having inner lists of different lengths.\n",
    "\n",
    "PyTorch tensors are optimized for doing neural network operations, and so they come with a few extra capabilities beyond `sum()`, `sort()`, `mean()`, etc.\n",
    "\n",
    "Let's start by importing them, and taking a look at how to work with multi-dimensional tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "from image_utils import open_image, make_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Shaping\n",
    "\n",
    "Let's open up an image and load its pixels into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg = open_image(\"./data/arara.jpg\")\n",
    "\n",
    "display(mimg)\n",
    "print(mimg.pixels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have to pass the list of pixels to the `Tensor()` constructor.\n",
    "\n",
    "We can check it's size with the `shape` member variable, and use slicing and indexing like we've always used with lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_t = Tensor(mimg.pixels)\n",
    "mimg_t.shape, mimg_t[:5], mimg_t[5], mimg_t[5][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this tensor is $607\\text{,}500 \\times 3$, meaning that we have $607\\text{,}500$ pixels and each pixel has $3$ color values.\n",
    "\n",
    "Let's reshape the tensor so it's more representative of our image's dimensions. We want to have a tensor of shape $h \\times w \\times 3$, where $h$ and $w$ are the images `height` and `width` dimensions.\n",
    "\n",
    "The `reshape()` function does just this, we just have to pass the parameters in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_t = Tensor(mimg.pixels).reshape(mimg.size[1], mimg.size[0], 3)\n",
    "\n",
    "mimg_t.shape, mimg_t[:5].shape, mimg_t[:5], mimg_t[0][5], mimg_t[0, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `mimg_t[:5]` doesn't refer to first $5$ pixels anymore, but to the first $5$ rows of our image.\n",
    "\n",
    "To get the first $5$ pixels we can use `mimg_t[0][:5]` or `mimg_t[0, :5]`.\n",
    "\n",
    "That's new syntax! using multiple numbers inside the square brackets, separated with a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_t[0][:5], mimg_t[0, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "This is where it starts to get fun.\n",
    "\n",
    "Since we now have our image in a $3D$ tensor, we can use slice in multiple directions, and at the same time.\n",
    "\n",
    "<img src=\"./imgs/slicing_00.jpg\" width=800px />\n",
    "\n",
    "#### Getting\n",
    "\n",
    "For example, if we want to crop a part of the image, we can just get slices in the first two dimensions, like this:\n",
    "\n",
    "`mimg_t[y0:y1, x0:x1]`\n",
    "\n",
    "where `x0` and `y0` are the horizontal and vertical location of the top-left pixel of the region we want, and `x1` and `y1` are the bottom-right coordinates of the last pixel we want.\n",
    "\n",
    "So, to grab a $256$ X $256$ section of an image, starting at $(x,y) = (240, 30)$ we can do:\n",
    "\n",
    "`mimg_crop = mimg_t[30:30+256, 240:240+256]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0,y0 = 240, 30\n",
    "\n",
    "mimg_crop_t = mimg_t[y0:y0+256, x0:x0+256]\n",
    "\n",
    "mimg_crop_t.shape, mimg_crop_t[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_crop = make_image(mimg_crop_t)\n",
    "display(mimg_crop)\n",
    "mimg_crop.pixels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting and Broadcasting\n",
    "\n",
    "Slicing also works when assigning values to regions of our tensor/image.\n",
    "\n",
    "Even if the values we're assigning don't perfectly match the region we want to assign them to, the tensor will try to _broadcast_ the value into the right places with the right shape.\n",
    "\n",
    "For example, we can assign a single pixel value to an entire region with:\n",
    "\n",
    "`mimg_t[y0:y1, x0:x1] = Tensor([220, 20, 120])`\n",
    "\n",
    "and it knows to set every pixel in that region the same color.\n",
    "\n",
    "Or, we can even do this, if we want to set a color in grayscale:\n",
    "\n",
    "`mimg_t[y0:y1, x0:x1] = 220`\n",
    "\n",
    "it will create a `Tensor([220, 220, 220])` to fill the pixel region specified.\n",
    "\n",
    "The tensor will convert/broadcast the value into the right shape to fit the region we are slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the tensor for editing\n",
    "mimg_blank_t = mimg_crop_t.clone()\n",
    "display(make_image(mimg_blank_t))\n",
    "\n",
    "mimg_blank_t[100:200, 100:200] = 0\n",
    "display(make_image(mimg_blank_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_blank_rows_t = mimg_crop_t.clone()\n",
    "\n",
    "# TODO: try to assign colors to entire rows/column\n",
    "\n",
    "display(make_image(mimg_blank_rows_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can separate the color channels of our images using a single line of code, and no looping!\n",
    "\n",
    "For looking at the `R` channel, just set `G` and `B` to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_crop_r_t = mimg_crop_t.clone()\n",
    "mimg_crop_r_t[:, :, 1:3] = 0\n",
    "\n",
    "# look at first 5 pixels\n",
    "mimg_crop_r_t[0, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://weeklydevotion.com/wp-content/uploads/2014/06/whoa.jpg\" height=200px /> <img src=\"https://sites.tufts.edu/emotiononthebrain/files/2014/10/tumblr_m0wb2xz9Yh1r08e3p.jpg\" height=200px />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_image(mimg_crop_r_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_crop_g_t = mimg_crop_t.clone()\n",
    "# TODO: get separate green channel image\n",
    "\n",
    "mimg_crop_b_t = mimg_crop_t.clone()\n",
    "# TODO: get separate blue channel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_image(mimg_crop_r_t))\n",
    "display(make_image(mimg_crop_g_t))\n",
    "display(make_image(mimg_crop_b_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing in Multiple Dimensions\n",
    "\n",
    "We can combine slicing regions and slicing specific color channels to create effects with little code.\n",
    "\n",
    "This creates an image by combining shifted versions of the separate `R`, `G` and `B` channel images from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an image the same shape as the original image, but with all 0s\n",
    "mimg_crop_rgb_t = mimg_crop_t.clone()\n",
    "mimg_crop_rgb_t[:] = 0\n",
    "\n",
    "mimg_crop_rgb_t[:, 32:, 0] += mimg_crop_t[:, :-32, 0]\n",
    "mimg_crop_rgb_t[:, :, 1] += mimg_crop_t[:, :, 1]\n",
    "mimg_crop_rgb_t[:, :-32, 2] += mimg_crop_t[:, 32:, 2]\n",
    "\n",
    "display(make_image(mimg_crop_rgb_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing Shape\n",
    "\n",
    "We can also get the individual pixel values for each channel using slicing.\n",
    "\n",
    "This gets all of the red values of all pixels as a two-dimensional tensor of shape $h$ X $w$:\n",
    "\n",
    "`mimg_crop_t[:,:,0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mimg_crop_t[:,:,0].shape)\n",
    "display(make_image(mimg_crop_t[:,:,0]))\n",
    "display(make_image(mimg_crop_t[:,:,1]))\n",
    "display(make_image(mimg_crop_t[:,:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations along specific dimensions\n",
    "\n",
    "Kind of like `DataFrames`, `Tensor` objects also have a bunch of built-in functions for performing common operations on their content.\n",
    "\n",
    "Functions like, `sum()`, `mean()`, `max()`, `std()`, should be familiar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_t = Tensor([[1, 2], [2, 4], [-2, -1]])\n",
    "print(my_t)\n",
    "print(my_t.sum(), my_t.mean(), my_t.max(), my_t.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `DataFrames` a lot of these functions would happen along columns, so we would get the `mean`, `max`, `sum` of each of the features in the dataset.\n",
    "\n",
    "By default our `Tensor` performs these operations on all of its data and returns one value.\n",
    "\n",
    "We can change this behavior by providing an extra argument to the functions, specifying the dimension along which we want to perform the operation. It helps to think of this parameter as the dimension we want to \"_reduce_\", or remove.\n",
    "\n",
    "So, for example, `sum(0)` gets rid of the rows, by summing down the `Tensor` columns, while `mean(1)`, gets rid of the columns, by computing the average value of the `Tensor` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_t)\n",
    "print(my_t.sum(0), my_t.mean(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that we can convert our image to grayscale in one line of code by reducing the $3^{rd}$ dimension, which holds the color values for each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_crop_gs_t = mimg_crop_t.mean(2)\n",
    "display(make_image(mimg_crop_gs_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering with Boolean Indexes\n",
    "\n",
    "We can also select certain elements, regions, or dimensions of our tensors using boolean tensors.\n",
    "\n",
    "Instead of passing numeric indexes, or slices, to our tensor's square brackets, we can select elements by passing a tensor of similar shape, but whose contents are `True`/`False` values.\n",
    "\n",
    "This works for setting and getting elements.\n",
    "\n",
    "The easiest way to create these boolean selector tensors is usually by manipulating the original tensor.\n",
    "\n",
    "The following line of code creates a two-dimensional tensor whose element are the difference between the `R` and `G` channels of our image:\n",
    "\n",
    "`(mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1])`\n",
    "\n",
    "It's first two dimensions are just like the original `mimg_crop_t`'s shape, but the last dimension holds a single value, and not a pixel value list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_rg_diff_t = mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1]\n",
    "\n",
    "print(mimg_rg_diff_t)\n",
    "print(mimg_rg_diff_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line creates a boolean tensor, whose values specify whether the `R` channel value is larger than the `G` channel value by more than $80$, for every pixel in the image:\n",
    "\n",
    "`((mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1]) > 80)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_rg_diff_thold_t = (mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1]) > 80\n",
    "\n",
    "print(mimg_rg_diff_thold_t)\n",
    "print(mimg_rg_diff_thold_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now select those pixels and multiple them by the one-dimensional tensor `[4, 1, 1]` to exaggerate their `R` channel values by a factor of $4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_red_bool_t = mimg_crop_t.clone()\n",
    "\n",
    "rgtg_idx = ((mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1]) > 80)\n",
    "mimg_red_bool_t[rgtg_idx] *= Tensor([4, 1, 1])\n",
    "\n",
    "display(make_image(mimg_red_bool_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Filtering\n",
    "\n",
    "Before running the cells... what do the following indexing, selecting, slicing, assignments do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this do?\n",
    "rgtg_idx = (mimg_crop_t[:,:,0] - mimg_crop_t[:,:,1]) > 80\n",
    "\n",
    "# what about this?\n",
    "rgtb_idx = (mimg_crop_t[:,:,0] - mimg_crop_t[:,:,2]) > 80\n",
    "\n",
    "# and these ?\n",
    "red_idx = rgtg_idx & rgtb_idx\n",
    "not_red_idx = ~red_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_idx_bool_t = mimg_crop_t.clone()\n",
    "mimg_idx_bool_t[not_red_idx] = 0\n",
    "\n",
    "display(make_image(mimg_idx_bool_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do these 2 lines do?\n",
    "mimg_blank_t = mimg_crop_t.clone()\n",
    "mimg_blank_t[:] = 0\n",
    "\n",
    "# how is this cell different from the ones above?\n",
    "mimg_blank_t[red_idx] = mimg_crop_t[red_idx]\n",
    "\n",
    "display(make_image(mimg_blank_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does this cell do that is different from the grayscale filter above?\n",
    "mimg_crop_gs_t = mimg_crop_t.mean(2)\n",
    "\n",
    "mimg_crop_rgb_gs_t = mimg_crop_t.clone()\n",
    "mimg_crop_rgb_gs_t[:,:,0] = mimg_crop_gs_t\n",
    "mimg_crop_rgb_gs_t[:,:,1] = mimg_crop_gs_t\n",
    "mimg_crop_rgb_gs_t[:,:,2] = mimg_crop_gs_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimg_gs_bool_t = mimg_crop_t.clone()\n",
    "\n",
    "# what does this do?\n",
    "mimg_gs_bool_t[not_red_idx] = mimg_crop_rgb_gs_t[not_red_idx]\n",
    "\n",
    "display(make_image(mimg_gs_bool_t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
